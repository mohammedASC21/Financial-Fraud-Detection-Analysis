{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5bd66ee",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In this notebook, we will ask you a series of questions regarding model selection. Based on your responses, we will ask you to create the ML models that you've chosen. \n",
    "\n",
    "The bonus step is completely optional, but if you provide a sufficient third machine learning model in this project, we will add `1000` points to your Kahoot leaderboard score.\n",
    "\n",
    "**Note**: Use the dataset that you've created in your previous data transformation step (not the original model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b90a0",
   "metadata": {},
   "source": [
    "## Questions\n",
    "Is this a classification or regression task?  \n",
    "\n",
    "This is a classification task since the target variable, isFraud, is categorical with two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bfb9f",
   "metadata": {},
   "source": [
    "Are you predicting for multiple classes or binary classes?  \n",
    "\n",
    "I am predicting for binary classes since isFraud only has two possible values, 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd9378",
   "metadata": {},
   "source": [
    "Given these observations, which 2 (or possibly 3) machine learning models will you choose?  \n",
    "\n",
    "Logistic Regression,\n",
    "Naive Bayes,\n",
    "K-Nearest Neighbors (KNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c408b67",
   "metadata": {},
   "source": [
    "## First Model\n",
    "\n",
    "Using the first model that you've chosen, implement the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b1f5bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fab3d0",
   "metadata": {},
   "source": [
    "### 1) Create a train-test split\n",
    "\n",
    "Use your cleaned and transformed dataset to divide your features and labels into training and testing sets. Make sure youâ€™re only using numeric or properly encoded features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b0c646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/transformed_transactions.csv\")\n",
    "\n",
    "# 3. Define features and target\n",
    "X = df.drop('isFraud', axis=1)\n",
    "y = df['isFraud']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c97f67",
   "metadata": {},
   "source": [
    "### 2) Search for best hyperparameters\n",
    "Use tools like GridSearchCV, RandomizedSearchCV, or model-specific tuning functions to find the best hyperparameters for your first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59c8fc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV - Best Params: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 10000, 'C': np.float64(0.48000000000000004)}\n",
      "RandomizedSearchCV - Cross-Val Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Randomly search for the best hyperparameters on a logistic regression model\n",
    "param_dist = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': np.linspace(0.01, 1, 100),\n",
    "    'solver': ['saga'], \n",
    "    'max_iter': [10000]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(LogisticRegression(), param_distributions=param_dist, cv=5, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model from random search\n",
    "best_params_random = random_search.best_params_\n",
    "best_score_random = random_search.best_score_\n",
    "\n",
    "print(f\"RandomizedSearchCV - Best Params: {best_params_random}\")\n",
    "print(f\"RandomizedSearchCV - Cross-Val Accuracy: {best_score_random:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed30eee3",
   "metadata": {},
   "source": [
    "### 3) Train your model\n",
    "Select the model with best hyperparameters and generate predictions on your test set. Evaluate your models accuracy, precision, recall, and sensitivity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "228ed831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV - Coefficients: [[-3.76340955e-06  1.79107653e-05 -2.26321116e-05  6.53035309e-06\n",
      "  -6.63268667e-06  2.95876825e-12 -4.03312285e-11 -2.63518958e-11\n",
      "  -1.06990901e-11 -4.40184976e-10  1.35391882e-10]]\n",
      "RandomizedSearchCV - Test Accuracy: 0.92\n",
      "Confusion Matrix:\n",
      "[[335  50]\n",
      " [ 14 380]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9599    0.8701    0.9128       385\n",
      "           1     0.8837    0.9645    0.9223       394\n",
      "\n",
      "    accuracy                         0.9178       779\n",
      "   macro avg     0.9218    0.9173    0.9176       779\n",
      "weighted avg     0.9214    0.9178    0.9176       779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the best model found from RandomizedSearchCV to predict on unseen test data\n",
    "\n",
    "# extract the best estimator\n",
    "best_log = random_search.best_estimator_\n",
    "\n",
    "# predict on testing data\n",
    "log_predictions = best_log.predict(X_test)\n",
    "\n",
    "# evaluate its accuracy\n",
    "test_score = accuracy_score(log_predictions, y_test)\n",
    "\n",
    "print(f\"RandomizedSearchCV - Coefficients: {best_log.coef_}\")\n",
    "print(f\"RandomizedSearchCV - Test Accuracy: {test_score:.2f}\")\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, log_predictions))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, log_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db128d64",
   "metadata": {},
   "source": [
    "## Second Model\n",
    "\n",
    "Create a second machine learning object and rerun steps (2) & (3) on this model. Compare accuracy metrics between these two models. Which handles the class imbalance more effectively?\n",
    "\n",
    "Create as many code-blocks as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "732baab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Gaussian NB classifier\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# generate predictions and display\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1685f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[375  10]\n",
      " [233 161]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.97      0.76       385\n",
      "           1       0.94      0.41      0.57       394\n",
      "\n",
      "    accuracy                           0.69       779\n",
      "   macro avg       0.78      0.69      0.66       779\n",
      "weighted avg       0.78      0.69      0.66       779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66c411",
   "metadata": {},
   "source": [
    "### (Bonus/Optional) Third Model\n",
    "\n",
    "Create a third machine learning model and rerun steps (2) & (3) on this model. Which model has the best predictive capabilities? \n",
    "\n",
    "Create as many code-blocks as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7c3f5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[347  38]\n",
      " [ 26 368]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92       385\n",
      "           1       0.91      0.93      0.92       394\n",
      "\n",
      "    accuracy                           0.92       779\n",
      "   macro avg       0.92      0.92      0.92       779\n",
      "weighted avg       0.92      0.92      0.92       779\n",
      "\n",
      "\n",
      "Accuracy: 0.9178433889602053\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81773f87",
   "metadata": {},
   "source": [
    "Naive Bayes performed worst out of all the three models which having 0.69 accuracy, precision of 0.94, Recall of 0.41, F1-Score of 0.57. It have high precision, but very low recall since it missed many frauds which shows it only flags obvious cases. Overall accuracy of 0.69 is misleading due to class imbalance.\n",
    "\n",
    "Both models Logistic regression and KNN perform equally well in accuracy with both having accuracy of 91.78%. KNN has better precision with fewer false alarms. Logistic Regression has higher recall kf 96.5% when KNN have 93% which mean Logistic regression catches more fraud compare to KNN. Logistic Regression is slightly better with the f1-scores too.\n",
    "\n",
    "Overall, Logistic regresstion is slightly better than KNN since it can catch as many frauds as possible due to it's higher recall.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
